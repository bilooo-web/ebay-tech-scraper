# eBay Tech Deals Scraper

This project is focused on building a complete data pipeline to scrape live product data from eBay Global Tech Deals, process the data, and perform exploratory data analysis (EDA). The project includes the following tasks:

1. **Web Scraping with Selenium**: Scrapes live product data from the eBay tech deals page.
2. **Automation with GitHub Actions**: Automatically updates the scraped data every three hours using GitHub Actions.
3. **Data Cleaning & Processing**: Cleans the scraped data and prepares it for analysis.
4. **Exploratory Data Analysis (EDA)**: Analyzes the cleaned data using various visualizations and statistics.

---

## Task 1: Web Scraping with Selenium

In this task, a Python script (`scraper.py`) was created to:

- Use **Selenium** to open the [eBay Tech Deals page](https://www.ebay.com/globaldeals/tech).
- Scroll down the page to trigger lazy loading of all product listings.
- Extract product details such as:
  - **timestamp** (scraping time)
  - **title** (product title)
  - **price** (discounted price)
  - **original_price** (original price, if available)
  - **shipping** (shipping details)
  - **item_url** (product URL)
- Save the extracted data into a CSV file (`ebay_tech_deals.csv`), appending new data if the file already exists.
- The script does not impose a limit on the number of products to scrape.

---

## Task 2: Automation with GitHub Actions

A **GitHub Actions** workflow was set up to automatically run the scraper every three hours for nearly two days. The cron expression used for scheduling the workflow is:
cron: '0 */3 * * *'

This ensures the scraper updates the CSV file (`ebay_tech_deals.csv`) at three-hour intervals, building a robust dataset.

---

## Task 3: Data Cleaning & Processing

The `clean_data.py` script was created to clean and process the raw data:

- Loads the raw CSV file (`ebay_tech_deals.csv`).
- Cleans the **price** and **original_price** columns by removing "$" and commas and converts them to numeric values.
- Replaces missing **original_price** values with the corresponding **price**.
- Cleans the **shipping** column by replacing missing values with "Shipping info unavailable".
- Creates a new column **discount_percentage** using the formula:

discount_percentage = ((original_price - price) / original_price) * 100


The cleaned data is saved as `cleaned_ebay_deals.csv`.

---

## Task 4: Exploratory Data Analysis (EDA)

The `EDA.ipynb` Jupyter Notebook performs the following analyses on the cleaned data:

### 1. Time Series Analysis
- Converts the **timestamp** column to datetime format and extracts the hour.
- Groups data by hour and visualizes the number of deals per hour.

### 2. Price and Discount Analysis
- Plots the distribution of product prices and discount percentages using histograms and boxplots.
- Creates a scatter plot comparing **original_price** vs **price**.

### 3. Shipping Information Analysis
- Counts the frequency of different shipping options.
- Plots a bar chart showing the frequency of shipping options.

### 4. Text Analysis on Product Titles
- Defines a set of keywords (e.g., "Apple", "Samsung", "Laptop") and counts the occurrences of these keywords in product titles.
- Visualizes keyword frequencies using a bar chart.

### 5. Price Difference Analysis
- Computes the **absolute discount** (original_price - price).
- Plots a histogram of price differences.

### 6. Top Discount Deals
- Sorts the dataset by **discount_percentage** in descending order.
- Displays the top 5 deals with the highest discounts.

---

## Submission Requirements

The repository should include the following files:

- `scraper.py`: The script for scraping data from eBay.
- `clean_data.py`: The script for cleaning and processing the data.
- `ebay_tech_deals.csv`: The raw CSV file generated by the scraper.
- `cleaned_ebay_deals.csv`: The cleaned CSV file after processing.
- `EDA.ipynb`: The Jupyter Notebook with EDA and visualizations.
- `.github/workflows/scraper.yml`: The GitHub Actions workflow for automating the scraper.

---

## Challenges Faced

- **Lazy Loading**: Handling lazy loading of content on the eBay page required careful use of Selenium to scroll down the page and trigger the loading of all products.
- **Automation**: Setting up GitHub Actions to run the scraper periodically and handle the data updates in a seamless manner.
- **Data Cleaning**: Cleaning and handling missing values for columns such as **original_price** and **shipping** was challenging, as there were multiple ways missing data was represented.
  
---

## Potential Improvements

- **Error Handling**: Adding more robust error handling to manage potential issues during scraping, such as page load failures or network interruptions.
- **Data Enrichment**: Incorporating additional features or external data sources, such as product ratings or seller information, could provide more insights.
- **Data Storage**: Using a database for storing large amounts of scraped data instead of a CSV file could improve scalability.

---

## Conclusion

This project demonstrates the process of scraping, cleaning, and analyzing eBay's tech deals data, along with automating the entire pipeline using GitHub Actions. The insights gained from the exploratory data analysis provide valuable information on pricing, discounts, and shipping options for eBay tech products.


